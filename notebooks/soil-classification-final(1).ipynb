{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-24T02:26:54.767259Z",
     "iopub.status.busy": "2025-05-24T02:26:54.767033Z",
     "iopub.status.idle": "2025-05-24T02:27:06.802678Z",
     "shell.execute_reply": "2025-05-24T02:27:06.802111Z",
     "shell.execute_reply.started": "2025-05-24T02:26:54.767241Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import required libraries                   \n",
    "import os\n",
    "import numpy as np                    \n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from sklearn.metrics import f1_score     \n",
    "from PIL import Image   \n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T02:27:06.803675Z",
     "iopub.status.busy": "2025-05-24T02:27:06.803324Z",
     "iopub.status.idle": "2025-05-24T02:27:06.868271Z",
     "shell.execute_reply": "2025-05-24T02:27:06.867580Z",
     "shell.execute_reply.started": "2025-05-24T02:27:06.803648Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the device to CPU or GPU based on availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T02:27:06.870337Z",
     "iopub.status.busy": "2025-05-24T02:27:06.870119Z",
     "iopub.status.idle": "2025-05-24T02:27:06.978760Z",
     "shell.execute_reply": "2025-05-24T02:27:06.978259Z",
     "shell.execute_reply.started": "2025-05-24T02:27:06.870320Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define paths to training and test folders\n",
    "train_dir = '/kaggle/input/soil-classification/soil_classification-2025/train'\n",
    "test_dir = '/kaggle/input/soil-classification/soil_classification-2025/test'\n",
    "\n",
    "# Load the CSV files with training labels and test image IDs\n",
    "train_df = pd.read_csv('/kaggle/input/soil-classification/soil_classification-2025/train_labels.csv')\n",
    "test_df = pd.read_csv('/kaggle/input/soil-classification/soil_classification-2025/test_ids.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T02:27:06.979558Z",
     "iopub.status.busy": "2025-05-24T02:27:06.979350Z",
     "iopub.status.idle": "2025-05-24T02:27:06.990751Z",
     "shell.execute_reply": "2025-05-24T02:27:06.990036Z",
     "shell.execute_reply.started": "2025-05-24T02:27:06.979542Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    'Alluvial soil': 0,\n",
    "    'Black Soil': 1,\n",
    "    'Clay soil': 2,\n",
    "    'Red soil': 3\n",
    "}\n",
    "\n",
    "inv_label_map = {v: k for k, v in label_map.items()}  # Inverse for predictions\n",
    "\n",
    "# Mapping labels to numeric classes\n",
    "\n",
    "train_df['label'] = train_df['soil_type'].map(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T02:27:06.991837Z",
     "iopub.status.busy": "2025-05-24T02:27:06.991515Z",
     "iopub.status.idle": "2025-05-24T02:27:07.005706Z",
     "shell.execute_reply": "2025-05-24T02:27:07.005021Z",
     "shell.execute_reply.started": "2025-05-24T02:27:06.991809Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Creating Dataset class for loading the image dataset\n",
    "class SoilDataset(Dataset):\n",
    "    def __init__(self, dataframe, img_dir, transform=None, is_test=False):\n",
    "        self.dataframe = dataframe\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.dataframe.iloc[idx, 0]\n",
    "        img_path = os.path.join(self.img_dir, image_id)\n",
    "        image = Image.open(img_path).convert('RGB')  \n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.is_test:\n",
    "            return image, image_id\n",
    "        else:\n",
    "            label = self.dataframe.iloc[idx, -1] \n",
    "            return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T02:27:07.006714Z",
     "iopub.status.busy": "2025-05-24T02:27:07.006391Z",
     "iopub.status.idle": "2025-05-24T02:27:07.021951Z",
     "shell.execute_reply": "2025-05-24T02:27:07.021464Z",
     "shell.execute_reply.started": "2025-05-24T02:27:07.006695Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to calculate the mean and standard deviation\n",
    "def calculate_mean_std(dataloader):\n",
    "    mean = 0\n",
    "    std = 0\n",
    "    total_images = 0\n",
    "    for images, _ in dataloader:\n",
    "        batch_size = images.size(0)\n",
    "        images = images.view(batch_size, images.size(1), -1)\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "        total_images += batch_size\n",
    "    mean /= total_images\n",
    "    std = torch.sqrt(std / (total_images * images.size(2)))\n",
    "    return mean,std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T02:27:07.022871Z",
     "iopub.status.busy": "2025-05-24T02:27:07.022627Z",
     "iopub.status.idle": "2025-05-24T02:27:07.041780Z",
     "shell.execute_reply": "2025-05-24T02:27:07.041115Z",
     "shell.execute_reply.started": "2025-05-24T02:27:07.022851Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create an initial data loader to determine the mean and standard deviation of the dataset\n",
    "initial_transform = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor()])\n",
    "initial_dataset = SoilDataset(train_df, train_dir, transform=initial_transform)\n",
    "initial_loader = DataLoader(initial_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T02:27:07.042753Z",
     "iopub.status.busy": "2025-05-24T02:27:07.042523Z",
     "iopub.status.idle": "2025-05-24T02:27:24.675278Z",
     "shell.execute_reply": "2025-05-24T02:27:24.674439Z",
     "shell.execute_reply.started": "2025-05-24T02:27:07.042737Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:tensor([0.5194, 0.4144, 0.3265])\n",
      "standard_deviation:tensor([0.0017, 0.0016, 0.0016])\n"
     ]
    }
   ],
   "source": [
    "mean,std = calculate_mean_std(initial_loader)\n",
    "print(f\"mean:{mean}\")\n",
    "print(f\"standard_deviation:{std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T02:27:24.677630Z",
     "iopub.status.busy": "2025-05-24T02:27:24.677342Z",
     "iopub.status.idle": "2025-05-24T02:27:24.682533Z",
     "shell.execute_reply": "2025-05-24T02:27:24.681880Z",
     "shell.execute_reply.started": "2025-05-24T02:27:24.677613Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Transforms for train data and test data\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((260, 260)),                    # Resizing input size\n",
    "    transforms.RandomHorizontalFlip(),                # Horizontal flips\n",
    "    transforms.RandomRotation(15),                    # Random rotation\n",
    "    transforms.ColorJitter(0.3, 0.3, 0.3),            # Color jitters\n",
    "    transforms.ToTensor(),                            # Converting tensor\n",
    "    transforms.Normalize(mean,        # Normalizing with ImageNet means and stds\n",
    "                         std)                    # Normalize the data by using the mean and std \n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([                 \n",
    "    transforms.Resize((260, 260)),                    # Resizing input size\n",
    "    transforms.ToTensor(),                            # Converting to tensor\n",
    "    transforms.Normalize(mean,        # Normalizing with ImageNet means and stds\n",
    "                         std)                    # Normalize the data by using the mean and std\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T02:27:24.683908Z",
     "iopub.status.busy": "2025-05-24T02:27:24.683280Z",
     "iopub.status.idle": "2025-05-24T02:27:24.699122Z",
     "shell.execute_reply": "2025-05-24T02:27:24.698456Z",
     "shell.execute_reply.started": "2025-05-24T02:27:24.683890Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Creating training and test dataset objects\n",
    "train_dataset = SoilDataset(train_df, train_dir, transform=train_transform)\n",
    "test_dataset = SoilDataset(test_df, test_dir, transform=test_transform, is_test=True)\n",
    "\n",
    "# Creating dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T02:27:24.700130Z",
     "iopub.status.busy": "2025-05-24T02:27:24.699830Z",
     "iopub.status.idle": "2025-05-24T02:27:26.050264Z",
     "shell.execute_reply": "2025-05-24T02:27:26.049714Z",
     "shell.execute_reply.started": "2025-05-24T02:27:24.700113Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B2_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b2_rwightman-c35c1473.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b2_rwightman-c35c1473.pth\n",
      "100%|██████████| 35.2M/35.2M [00:00<00:00, 196MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Loading pretrained EfficientNet-B2 model\n",
    "model = models.efficientnet_b2(pretrained=True)\n",
    "\n",
    "# Modify the classification layer to classify 4 soil types\n",
    "model.classifier[1] = nn.Linear(model.classifier[1].in_features, 4)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T02:27:26.051378Z",
     "iopub.status.busy": "2025-05-24T02:27:26.051108Z",
     "iopub.status.idle": "2025-05-24T02:27:26.057008Z",
     "shell.execute_reply": "2025-05-24T02:27:26.056470Z",
     "shell.execute_reply.started": "2025-05-24T02:27:26.051355Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Creating the loss function of CrossEntropy with label smoothening to avoid overfitting\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Creating the optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Creating scheduler to control the learning rate\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T02:27:26.058042Z",
     "iopub.status.busy": "2025-05-24T02:27:26.057714Z",
     "iopub.status.idle": "2025-05-24T02:27:26.074189Z",
     "shell.execute_reply": "2025-05-24T02:27:26.073586Z",
     "shell.execute_reply.started": "2025-05-24T02:27:26.058020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training function to determine the best weights based on highest least f1 score\n",
    "def train_model(model, train_loader, epochs=20):\n",
    "    final_weights = None\n",
    "    best_f1 = 0.0# Save the best minumum F1 score\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch no: {epoch}\")\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_loss += loss.item()\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Calculate class-wise F1-scores\n",
    "        all_f1 = f1_score(all_labels, all_preds, average=None)\n",
    "        min_f1 = min(all_f1)\n",
    "        print(f\"Epoch:{epoch+1} Loss:{running_loss:.4f} Min F1:{min_f1:.4f}\")\n",
    "\n",
    "        # Step the LR scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # Save the best model\n",
    "        if min_f1 > best_f1:\n",
    "            best_f1 = min_f1\n",
    "            final_weights = model.state_dict()\n",
    "            print(\"New model saved!\")\n",
    "\n",
    "    # Load best model before returning\n",
    "    model.load_state_dict(final_weights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T02:27:26.075055Z",
     "iopub.status.busy": "2025-05-24T02:27:26.074835Z",
     "iopub.status.idle": "2025-05-24T02:34:45.888892Z",
     "shell.execute_reply": "2025-05-24T02:34:45.888104Z",
     "shell.execute_reply.started": "2025-05-24T02:27:26.075040Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch no: 0\n",
      "Epoch:1 Loss:33.6602 Min F1:0.6290\n",
      "New model saved!\n",
      "Epoch no: 1\n",
      "Epoch:2 Loss:12.1871 Min F1:0.8514\n",
      "New model saved!\n",
      "Epoch no: 2\n",
      "Epoch:3 Loss:7.1844 Min F1:0.8889\n",
      "New model saved!\n",
      "Epoch no: 3\n",
      "Epoch:4 Loss:5.2068 Min F1:0.9231\n",
      "New model saved!\n",
      "Epoch no: 4\n",
      "Epoch:5 Loss:4.2166 Min F1:0.9495\n",
      "New model saved!\n",
      "Epoch no: 5\n",
      "Epoch:6 Loss:3.6166 Min F1:0.9526\n",
      "New model saved!\n",
      "Epoch no: 6\n",
      "Epoch:7 Loss:3.5383 Min F1:0.9761\n",
      "New model saved!\n",
      "Epoch no: 7\n",
      "Epoch:8 Loss:3.6434 Min F1:0.9548\n",
      "Epoch no: 8\n",
      "Epoch:9 Loss:2.1310 Min F1:0.9784\n",
      "New model saved!\n",
      "Epoch no: 9\n",
      "Epoch:10 Loss:1.9161 Min F1:0.9776\n",
      "Epoch no: 10\n",
      "Epoch:11 Loss:1.9557 Min F1:0.9924\n",
      "New model saved!\n",
      "Epoch no: 11\n",
      "Epoch:12 Loss:1.6594 Min F1:0.9849\n",
      "Epoch no: 12\n",
      "Epoch:13 Loss:1.7815 Min F1:0.9900\n",
      "Epoch no: 13\n",
      "Epoch:14 Loss:1.8431 Min F1:0.9870\n",
      "Epoch no: 14\n",
      "Epoch:15 Loss:1.1713 Min F1:0.9924\n",
      "Epoch no: 15\n",
      "Epoch:16 Loss:1.5167 Min F1:0.9899\n",
      "Epoch no: 16\n",
      "Epoch:17 Loss:1.0257 Min F1:0.9914\n",
      "Epoch no: 17\n",
      "Epoch:18 Loss:0.7773 Min F1:0.9935\n",
      "New model saved!\n",
      "Epoch no: 18\n",
      "Epoch:19 Loss:1.4610 Min F1:0.9824\n",
      "Epoch no: 19\n",
      "Epoch:20 Loss:1.0119 Min F1:0.9892\n"
     ]
    }
   ],
   "source": [
    "# Epochs and training the model\n",
    "model = train_model(model, train_loader, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T02:34:45.890224Z",
     "iopub.status.busy": "2025-05-24T02:34:45.889933Z",
     "iopub.status.idle": "2025-05-24T02:34:50.879145Z",
     "shell.execute_reply": "2025-05-24T02:34:50.878474Z",
     "shell.execute_reply.started": "2025-05-24T02:34:45.890198Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission generated\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on test dataset\n",
    "def predict_and_generate_submission(model):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    image_ids = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, ids in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            image_ids.extend(ids)\n",
    "\n",
    "    # Convert numerical predictions back to soil labels\n",
    "    label_map = {\n",
    "        0:'Alluvial soil',\n",
    "        1:'Black Soil',\n",
    "        2:'Clay soil',\n",
    "        3:'Red soil'\n",
    "    }\n",
    "    pred_labels = [inv_label_map[p] for p in predictions]\n",
    "    return pd.DataFrame({'image_id': image_ids, 'soil_type': pred_labels})\n",
    "\n",
    "# Create submission.csv\n",
    "submission = predict_and_generate_submission(model)\n",
    "submission.to_csv('/kaggle/working/submission.csv', index=False)\n",
    "print(\"submission generated\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 12375409,
     "sourceId": 102672,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
